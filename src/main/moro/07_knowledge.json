{
  "name" : "Inclusion of Prior Knowledge in Factorization Models",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "drawbacks",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Drawbacks of OpenIE and Embeddings\n\n<br />\n<img src=\"../../assets/figures/07/MF-7.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "drawbacks1",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Drawbacks of OpenIE and Embeddings\n\n<br />\n<img src=\"../../assets/figures/07/MF-14.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "drawbacksTODO",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### TODO: add intermediate slides\n\n<br />\n<img src=\"../../assets/figures/07/MF-16.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "logic",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### First-order Logic Prior Knowledge\n\n- *Representation Learning*: hard to fix mistakes\n\n    \\\\(\\neg \\text{person}(x) \\Rightarrow \\neg \\text{place of birth}(x,y)\\\\) \n- *Distant Supervision*: fails for no or little alignment\n\n    \\\\(\\text{#1-is-a-student-in-#2-'s-lab}(x,y) \\Rightarrow \\text{supervisedBy}(x,y)\\\\)\n- Pros: Formulae are easy to modify and improve\n- Cons: Brittle, no generalization and inference can become intractable\n- Markov Logic Networks: easy to modify, generalize well, but inference often intractable in practice",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "overview",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Low-rank Logic Embeddings \n\n<br />\n<img src=\"../../assets/figures/07/Overview-7.png\" height=\"300\">",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "mflogic",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Combining Logic and Matrix Factorization\n\n<br />\n<img src=\"../../assets/figures/07/Baselines-14.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 12,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "injectfacts",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Matrix Factorization is \"Injecting\" Atomic Formulae\n\n- Training facts are ground atoms: \\\\(\\mathcal{F} = r_s(e_i, e_j)\\\\)\n- Let [•] denote mapping from symbolic formulae to expectations\n- \\\\([r_s(e_i, e_j)] := \\sigma(\\mathbf{v}_{s} \\cdot \\mathbf{v}_{ij})\\\\)\n- Training objective:\n\n    \\\\(\\max_{\\{\\mathbf{v}_s\\}, \\{\\mathbf{v}_{ij}\\}} \\sum_{\\mathcal{F} \\in \\mathfrak{F}} \\log([\\mathcal{F}])\\\\)\n- Can we do this for any propositional formulae? \n  \n    \\\\([\\mathcal{F}] = [r_1(e_i\\,e_j) \\wedge \\neg r_2(e_i\\,e_j) \\Rightarrow r_3(e_i\\,e_j)]\\\\)\n",
      "extraFields" : { }
    }
  }, {
    "id" : 14,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "difflogic",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Differentiable Logic Formulae\n- \\\\([ \\mathcal{F} ] = \\\\)\n    - \\\\(\\sigma(\\mathbf{v}_s \\cdot \\mathbf{v}_{ij}) \\qquad\\text{ if } \\mathcal{F} = r_s(e_i, e_j), \\text{ i.e., facts}\\\\)\n    - \\\\(1 - [\\mathcal{A}] \\;\\;\\qquad\\text{ if } \\mathcal{F} = \\neg\\mathcal{A}\\\\)\n    - \\\\([\\mathcal{A}] * [\\mathcal{B}] \\qquad\\text{ if } \\mathcal{F} = \\mathcal{A} \\wedge \\mathcal{B}\\\\)\n  \n- From negation and conjunction we can build any propositional formula\n    - Disjunction:\n        \\\\([\\mathcal{A} \\vee \\mathcal{B}] = 1 - (1 - [\\mathcal{A}])*(1 - [\\mathcal{B}])\\\\)\n    - Implication:\n        \\\\([\\mathcal{A} \\Rightarrow \\mathcal{B}] = [\\mathcal{A}]([\\mathcal{B}] - 1) + 1\\\\)\n- Jointly maximize the log-likelihood of atomic and propositional formulae\n      \\\\(\\max_{\\{\\mathbf{v}_s\\}, \\{\\mathbf{v}_{ij}\\}} \\sum_{\\mathcal{F} \\in \\mathfrak{F}} \\log([\\mathcal{F}])\\\\)",
      "extraFields" : { }
    }
  }, {
    "id" : 16,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "backprop",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Backpropagation Through Structure\n(Goller and Küchler, 1996)\n\n<br />\n<img src=\"../../assets/figures/07/Backprop.png\" height=\"400\">",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "grounding",
      "extraFields" : { }
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Grounding\n\n- \\\\(\\forall x,y: r_s(x,y) \\Rightarrow r_t(x,y)\\\\)\n- Grounding based on all observed facts for premise \\\\(r_s(e_i,e_j)\\\\) and consequent \\\\(r_t(e_i,e_j)\\\\)\n- Sample unobserved facts \\\\(r_s(e_i', e_j')\\\\) and \\\\(r_t(e_i', e_j')\\\\)\n- Add propositional formulae to the matrix factorization training objective\n    - \\\\([r_s(e_i,e_j) \\Rightarrow r_t(e_i,e_j)]\\\\)\n    - \\\\([r_s(e_i',e_j') \\Rightarrow r_t(e_i',e_j')]\\\\)\n",
      "extraFields" : { }
    }
  }, {
    "id" : 20,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "experiments",
      "extraFields" : { }
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Expriments\n- Relation extraction corpus (Riedel et al., 2013)\n    - ~4k textual patterns from New York Times corpus and 151 Freebase relations, ~42k entity-pairs, ~100k training facts \n    - Metric: mean average precision (*MAP*) on manually annotated predictions for Freebase relations\n- *Zero-shot Relation Learning*: \n    - All Freebase training facts are removed\n    - No alignment between Freebase relations and textual patterns\n- *Relations with Few Distant Labels*:\n    - Varying degree of alignment between relations and textual patterns\n",
      "extraFields" : { }
    }
  }, {
    "id" : 22,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "evaluation",
      "extraFields" : { }
    }
  }, {
    "id" : 23,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Evaluation\n- Formulae extracted from predictions of a matrix factorization model (Sanchez et al., 2015)\n- Annotated manually\n- Given these formulae, which method can best make use of them?\n    - Logical Inference\n    - Post-Factorization Inference\n    - Pre-Factorization Inference\n    - Joint Optimization\n- Example: \n    \\\\(\\forall x,y: \\verb~#2-minister-#1~(x,y) \\Rightarrow \\verb~person/nationality~(x,y)\\\\)",
      "extraFields" : { }
    }
  }, {
    "id" : 24,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "results",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Zero-shot Relation Learning\n\n<br />\n<img src=\"../../assets/figures/07/zero-shot-5.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 26,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "fewdistant",
      "extraFields" : { }
    }
  }, {
    "id" : 27,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Relations with Few Distant Labels\n\n<br />\n<img src=\"../../assets/figures/07/results.png\" height=\"500\">",
      "extraFields" : { }
    }
  }, {
    "id" : 28,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "summary",
      "extraFields" : { }
    }
  }, {
    "id" : 29,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Summary\n- Matrix factorization for relation extraction generalizes well\n    - Hard to fix mistakes\n    - Fails for new or spare relations\n- Formalize background knowledge as logical formulae\n    - Brittle and no generalization\n- Formulae can be injected into embeddings\n    - Make logical background knowledge differentiable\n    - Jointly optimize over factual and first-order knowledge\n        - Learns relations with no or little prior information in databases\n        - Generalizes beyond textual patterns mentioned in formulae\n        - Joint optimization > Pre-factorization inference > Post-factorization inference > Logical inference",
      "extraFields" : { }
    }
  }, {
    "id" : 30,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "reference",
      "extraFields" : { }
    }
  }, {
    "id" : 31,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## References\n- Goller, Christoph, and Andreas Kuchler. \"Learning task-dependent distributed representations by backpropagation through structure.\" Neural Networks, 1996., IEEE International Conference on. Vol. 1. IEEE, 1996.",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
