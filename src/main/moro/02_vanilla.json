{
  "name" : "Vanilla Matrix Factorization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "completion",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "html",
    "input" : {
      "sessionId" : null,
      "code" : "<h3>Matrix Completion via Low-Rank Factorization</h3>\n<br>\n<div style=\"text-align:center;\">\nGiven \\(\\mathbf{Y} \\in\\Re^{N\\times M} \\) <br>\n<br>\nfind \\(\\mathbf{U} \\in\\Re^{N\\times L}\\) and \\(\\mathbf{V} \\in\\Re^{M\\times L} \\) <br>\n<br>\nso that \\(\\mathbf{Y} \\approx \\mathbf{U}\\mathbf{V}^{T}\\)<br>\n<br>\n<b>low rank</b> assumption: \\( rank(\\mathbf{Y})=L\\lt\\lt M,N \\)\n</div>",
      "extraFields" : { }
    }
  }, {
    "id" : 12,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "whyApp",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Why Low-Rank\n<br/>\n\n- low-rank assumption usually does not hold \n- reconstruction unlikely to be perfect\n<br/>\n<br/>\n\nBut:\n<br/>\n\n> original matrix exhibits **noise** and **redundancy**,<br/> low-rank reconstruction addresses these issues\n",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "why",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Why Low-Rank\n<br>\nWe want: A probabilistic model on matrices with:\n<br>\n<br>\n\n1. Invariance by permutation of rows and columns\n    \n    --> The model depends only on the eigenvalues\n\n2. A complexity measure\n\n    --> A norm or pseudo-norm (\\\\(e.g. L_\\alpha, \\alpha\\ge 0\\\\) ) on the spectrum \n\n3. Fast to compute and memory efficient\n\n    --> \\\\(\\alpha\\le 1\\\\) (Sparsity-inducing)",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "factorize",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Four ways to Factorize",
      "extraFields" : { }
    }
  }, {
    "id" : 16,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "svd",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Singular Value Decomposition (SVD)\n<br>\nGiven \\\\(\\mathbf{Y} \\in\\Re^{N\\times M} \\\\) there exists \n\n<br>\n$$\n\\mathbf{Y} = \\mathbf{U}\\mathbf{D}\\mathbf{V}\n$$\n<br>\nwhere \\\\(\\mathbf{U}\\in\\Re^{N\\times N},\\mathbf{V}\\in\\Re^{M\\times M}\\\\) are orthogonal<br/>\nand \\\\(\\mathbf{D} \\in\\Re^{N\\times M} \\ge 0\\\\) is diagonal\n\n<br/>\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "If we truncate \\\\(\\mathbf{D}\\\\) to its \\\\(L\\\\) largest values \n\nthen \\\\( \\hat{\\mathbf{Y}} = \\mathbf{U}\\mathbf{D_{trunc}}\\mathbf{V}\\\\)\n\nis the minimizer of \\\\(\\||\\mathbf{Y} - \\hat{\\mathbf{Y}}\\||_F^2\\\\)\n\n<br/>\n\n<div style=\"text-align:right;\">\n<i>Eckart-Young theorem</i>\n</div>",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 6,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "sgd",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### MF with Stochastic Gradient Descent\n<br>\nObjective:\n\\\\(\n\\gamma(U,V) := \\sum_{(i,j)\\in\\Omega} (y_{ij} - \\langle\\mathbf{u}_i,\\mathbf{v}_j\\rangle)^2 \n\\\\)\n\nwhere \\\\(\\Omega=\\lbrace 1,\\cdots,N\\rbrace \\otimes \\lbrace1,\\cdots,M\\rbrace \\\\)\n\n\\\\((i,j)\\notin \\Omega\\\\): **missing values**\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 20,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<br>\n**Algorithm**:\n\n1. Pick \\\\( (i,j)\\in\\Omega \\\\) uniformly at random\n\n2. Gradient steps\n\n\\\\(\\mathbf{u}_i \\leftarrow \\mathbf{u}_i - \\eta (\\langle\\mathbf{u}_i,\\mathbf{v}_j\\rangle-y_{ij}) \\mathbf{v}_j  \\\\)\n\n\\\\(\\mathbf{v}_j \\leftarrow \\mathbf{v}_j - \\eta (\\langle\\mathbf{u}_i,\\mathbf{v}_j\\rangle-y_{ij}) \\mathbf{u}_i \\\\)\n\nwhere \\\\(\\eta\\\\) is the gradient step size (can be adaptive)",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 8,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "als",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Alternating Least Squares\n<br>\nObjective:\n\\\\(\n\\gamma(U,V) := \\||\\mathbf{Y} - \\mathbf{U}\\mathbf{V}^{T}\\||_F^2 \n\\\\)\n\n<br>\n**block-coordinate descent**:\n\n\\\\(\n\\arg\\min_{\\mathbf{u}_i} \\gamma(U,V) = \\arg\\min_{u_i} \\||\\mathbf{Y}_{i:} - \\mathbf{u}_i\\mathbf{V}^{T}\\||_F^2 \n\\\\)\n\n\\\\(\n\\arg\\min_{\\mathbf{v}_j} \\gamma(U,V) = \\arg\\min_{v_j} \\||\\mathbf{Y}_{:j} - \\mathbf{U}\\mathbf{v_j}^{T}\\||_F^2 \n\\\\)\n\n<br>\nLeast square problems! ==> Unique solutions:\n\n\\\\(\n\\mathbf{u}_i \\leftarrow (\\mathbf{V}^T\\mathbf{V})^{-1} \\mathbf{V}^T \\mathbf{Y}_{i:} \n\\\\)\n\n\n\\\\(\n\\mathbf{v}_j \\leftarrow (\\mathbf{U}^T\\mathbf{U})^{-1} \\mathbf{U}^T \\mathbf{Y}_{:j} \n\\\\)\n<br>\nEach step can be parallelized efficiently",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "rlsa",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Regularized Latent Semantic Indexing\n<br>\nObjective:\n\\\\(\n\\gamma(U,V) := \\||\\mathbf{Y} - \\mathbf{U}\\mathbf{V}^{T}\\||_F^2 + \\lambda_1||\\mathbf{U}||_F^2 + \\lambda_2||\\mathbf{V}||_1\n\\\\)\n\nL2 for compact document-topic matrix \\\\(\\mathbf{U}\\\\) \n\nL1 for sparse term-topic matrix \\\\(\\mathbf{V}\\\\)\n\n<br>\n**block-coordinate descent**:\n\n\\\\(\n\\arg\\min_{\\mathbf{u}_i} \\gamma(U,V) = \\arg\\min_{u_i} \\||\\mathbf{Y}_{i:} - \\mathbf{u}_i\\mathbf{V}^{T}\\||_F^2\n+ \\lambda_1||\\mathbf{u_i}||_F^2\n\\\\)\n\n\\\\(\n\\arg\\min_{\\mathbf{v}_j} \\gamma(U,V) = \\arg\\min_{v_j} \\||\\mathbf{Y}_{:j} - \\mathbf{U}\\mathbf{v_j}^{T}\\||_F^2 \n+ \\lambda_2||\\mathbf{v_j}||_1\n\\\\)\n\n<br>\n\n\\\\(\n\\mathbf{u}_i \\leftarrow (\\mathbf{V}^T\\mathbf{V} + \\lambda_1\\mathbf{I})^{-1} \\mathbf{V}^T \\mathbf{Y}_{i:} \n\\\\) (ridge regression)\n\n\\\\(\\mathbf{v}_j\\\\) not differentiable, but can apply co-ordinate descent\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
