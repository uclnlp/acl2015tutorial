{
  "name" : "Vanilla Matrix Factorization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "completion",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "###Matrix Completion via Low-Rank Factorization\n\n<img src=\"../../assets/figures/vanillaMF.png\" height=\"300\">\n\nGiven \\\\(\\mathbf{Y} \\in\\Re^{N\\times M} \\\\)\n\nfind \\\\(\\mathbf{U} \\in\\Re^{N\\times L}\\\\) and \\\\(\\mathbf{V} \\in\\Re^{M\\times L}\\\\)\n\nso that \\\\(\\mathbf{Y} \\approx \\mathbf{U}\\mathbf{V}^{T}\\\\)\n\n<b>low rank</b> assumption: \\\\( rank(\\mathbf{Y})=L\\lt\\lt M,N \\\\)",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "whyApp",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Why Low-Rank\n\n- low-rank assumption usually does not hold\n\n- reconstruction unlikely to be perfect\n\n- if full-rank then perfect reconstruction is trivial: \\\\( \\mathbf{Y} = \\mathbf{Y}\\mathbf{I} \\\\)\n\n\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "&nbsp;&nbsp;&nbsp;&nbsp;Key insight:\n\n> original matrix has **redundancy** and **noise**, low-rank exploits the former to remove the latter",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 5,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "Demo",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import uk.ac.ucl.cs.mr.acltutorial.ManualMF._\nimport uk.ac.ucl.cs.mr.acltutorial.MatrixRenderer._\nimport uk.ac.ucl.cs.mr.acltutorial.Renderer._\nimport cc.factorie.la.{DenseTensor2, DenseTensor1}\nimport ml.wolfe.{Mat, Vect}\nval layout = Layout(cw = 80, ch = 50, colHeaderSize = 150, rowHeaderSize=100)\n\ndef optimizeL2(M: Mat, K: Int, iters: Int, alpha:Double = 0.1, initScale:Double = 1.0): (Seq[Vect], Seq[Vect]) = {\n  val rand = new scala.util.Random(0)\n  val AV = initialAV(K, M.dim1, M.dim2, initScale)\n  val A = AV._1; val V = AV._2\n  def update(i: Int, j: Int) = {\n    val a = A(i).copy\n    val v = V(j).copy\n    val y = a dot v\n    A(i) += v * alpha * (M(i, j) - y)\n    V(j) += a * alpha * (M(i, j) - y)\n  }\n  for (i <- Range(0,iters).toList) {\n    update(rand.nextInt(M.dim1), \n           rand.nextInt(M.dim2))\n  }\n  (A, V)\n}",
      "extraFields" : {
        "hide_output" : "true",
        "showEditor" : "false",
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 7,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val Y = parseMatrix(\n    \"\"\"1 2 0 0 0\n       0 0 1 0 0\n       0 0 0 1 1 \n       0 0 1 1 1\n       1 2 1 1 0\"\"\")\n       \nval rowNames = Seq(\"doc1\",\"doc2\",\"doc3\",\"doc4\",\"doc5\")\nval colNames = Seq(\"Greece\",\"Tsipras\",\"Germany\",\"crisis\",\"economy\")\nval names = header(rowNames,colNames)\nval rls = Matrix(hRulers = Seq(0,5), vRulers = Seq(0,5))\n       \nval m0 = numbers(matrix(Y)) + names +rls\nval m1 = opacity(matrix(Y), 0,2) + names + rls\nval m2 = Matrix(Seq(Cell(2,2,\"Y\")), hRulers = Seq(5), vRulers = Seq(5))\nval m3 = Matrix(Seq(Cell(6,2,\"V\"),Cell(2,6,\"U\"), Cell(2,2,\"Y\")), hRulers = Seq(5), vRulers = Seq(5))\nval (u,v) = optimizeL2(Y,2,1000)\nval m4 = numbers(embeddings(u,v)) +names\nval Yhat = dots(u,v)\nval m5 = numbers(matrix(Yhat)) + numbers(embeddings(u,v)) + names\nval m6 = opacity(matrix(Yhat),0,2) + numbers(embeddings(u,v)) + names\n\nrender(Seq(m0,m1,m3,m4,m5,m6,m1),layout) ",
      "extraFields" : {
        "showEditor" : "false",
        "aggregatedCells" : "[\"import uk.ac.ucl.cs.mr.acltutorial.ManualMF._\\nimport uk.ac.ucl.cs.mr.acltutorial.MatrixRenderer._\\nimport uk.ac.ucl.cs.mr.acltutorial.Renderer._\\nimport cc.factorie.la.{DenseTensor2, DenseTensor1}\\nimport ml.wolfe.{Mat, Vect}\\nval layout = Layout(cw = 80, ch = 50, colHeaderSize = 150, rowHeaderSize=100)\\n\\ndef optimizeL2(M: Mat, K: Int, iters: Int, alpha:Double = 0.1, initScale:Double = 1.0): (Seq[Vect], Seq[Vect]) = {\\n  val rand = new scala.util.Random(0)\\n  val AV = initialAV(K, M.dim1, M.dim2, initScale)\\n  val A = AV._1; val V = AV._2\\n  def update(i: Int, j: Int) = {\\n    val a = A(i).copy\\n    val v = V(j).copy\\n    val y = a dot v\\n    A(i) += v * alpha * (M(i, j) - y)\\n    V(j) += a * alpha * (M(i, j) - y)\\n  }\\n  for (i <- Range(0,iters).toList) {\\n    update(rand.nextInt(M.dim1), \\n           rand.nextInt(M.dim2))\\n  }\\n  (A, V)\\n}\"]"
      }
    }
  }, {
    "id" : 8,
    "compiler" : "html",
    "input" : {
      "sessionId" : null,
      "code" : "<div style=\"position:relative; width:600px; height:100px; margin:auto; display:block\">\n<div class=\"fragment0 fade-out\" style=\"position:absolute; top:0;left:0px;\">Document-Term matrix \\(\\mathbf{Y}\\)</div>\n<div class=\"fragment0 current-visible\" style=\"position:absolute; top:0;left:0px;\">Document-Term matrix \\(\\mathbf{Y}\\) <br> Rank?/topics?</div>\n<div class=\"fragment1 current-visible\" style=\"position:absolute; top:0;left:0px;\">Document-Term matrix \\(\\mathbf{Y}\\) <br/>Document-Topic \\(\\mathbf{U}\\), Topic-Term \\(\\mathbf{V}\\)</div>\n<div class=\"fragment2 current-visible\" style=\"position:absolute; top:0;left:0px;\"> <br/>Document-Topic \\(\\mathbf{U}\\), Topic-Term \\(\\mathbf{V}\\)</div>\n<div class=\"fragment3 current-visible\" style=\"position:absolute; top:0;left:0px;\">Reconstructed \\(\\mathbf{Y}\\) <br/>Document-Topic \\(\\mathbf{U}\\), Topic-Term \\(\\mathbf{V}\\)</div>\n<div class=\"fragment4 current-visible\" style=\"position:absolute; top:0;left:0px;\">Reconstructed \\(\\mathbf{Y}\\) <br/>Document-Topic \\(\\mathbf{U}\\), Topic-Term \\(\\mathbf{V}\\)</div\n><div class=\"fragment5 current-visible\" style=\"position:absolute; top:0;left:0px;\">Document-Term matrix \\(\\mathbf{Y}\\)</div>\n</div>",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "why",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Why Low-Rank\n\nBecause we want a model on matrices with:\n\n1. Invariance by permutation of rows and columns\n> The order of instances and features is irrelevant\n\n2. A measure to control complexity\n> A \\\\(p\\\\)-norm with \\\\(p \\ge 0\\\\) \n\n3. Fast to compute and memory efficient\n> \\\\(p\\le 1\\\\) (sparsity-inducing)",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "factorize",
      "extraFields" : { }
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Three ways to Factorize",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "svd",
      "extraFields" : { }
    }
  }, {
    "id" : 14,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Singular Value Decomposition (SVD)\n\nGiven \\\\(\\mathbf{Y} \\in\\Re^{M\\times N} \\\\) there exists \n\n$$\n\\mathbf{Y} = \\mathbf{U}\\mathbf{D}\\mathbf{V}\n$$\n\nwhere \\\\(\\mathbf{U}\\in\\Re^{M\\times M},\\mathbf{V}\\in\\Re^{N\\times N}\\\\) are orthogonal (\\\\(\\mathbf{U}\\mathbf{U}^T = \\mathbf{V}\\mathbf{V}^T= I\\\\))<br/>\nand \\\\(\\mathbf{D} \\in\\Re^{M\\times N} \\ge 0\\\\) is diagonal\n\n<img src=\"../../assets/figures/SVD.png\" height=\"300\">",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "truncsvd",
      "extraFields" : { }
    }
  }, {
    "id" : 16,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Truncated SVD\n\nIf we truncate \\\\(\\mathbf{D}\\\\) to its \\\\(L\\\\) largest values \n\n<img src=\"../../assets/figures/SVDtrunc.png\" height=\"300\">\n\nthen \\\\( \\hat{\\mathbf{Y}} = \\mathbf{U}\\mathbf{D_{trunc}}\\mathbf{V}\\\\)\n\nis the rank-\\\\(L\\\\) minimizer of \\\\(\\||\\mathbf{Y} - \\hat{\\mathbf{Y}}\\||_F^2\\\\)\n\n<div style=\"text-align:right;\">\n<i>Eckart-Young theorem</i>\n</div>",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "sgd",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### MF with Stochastic Gradient Descent\n\nObjective:\n\\\\(\n\\gamma(U,V) := \\sum_{(i,j)\\in\\Omega} (y_{ij} - \\langle\\mathbf{u}_i,\\mathbf{v}_j\\rangle)^2 \n\\\\)\n\n\\\\((i,j)\\notin \\Omega\\\\): **missing values**\n",
      "extraFields" : { }
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "**Algorithm**:\n\n1. Pick \\\\( (i,j)\\in\\Omega \\\\) uniformly at random\n\n2. Gradient steps\n\n\\\\(\\mathbf{u}_i \\leftarrow \\mathbf{u}_i - \\eta (\\langle\\mathbf{u}_i,\\mathbf{v}_j\\rangle-y_{ij}) \\mathbf{v}_j  \\\\)\n\n\\\\(\\mathbf{v}_j \\leftarrow \\mathbf{v}_j - \\eta (\\langle\\mathbf{u}_i,\\mathbf{v}_j\\rangle-y_{ij}) \\mathbf{u}_i \\\\)\n\nwhere \\\\(\\eta\\\\) is the gradient step size (can be adaptive)",
      "extraFields" : { }
    }
  }, {
    "id" : 20,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "als",
      "extraFields" : { }
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Alternating Least Squares\n\nObjective:\n\\\\(\n\\gamma(U,V) := \\||\\mathbf{Y} - \\mathbf{U}\\mathbf{V}^{T}\\||_F^2 \n\\\\)\n\n**block-coordinate descent**:\n\n\\\\(\n\\arg\\min_{\\mathbf{u}_{i:}} \\gamma(U,V) = \\arg\\min_{\\mathbf{u}_{i:}} \\||\\mathbf{y}_{i:} - \\mathbf{V}\\mathbf{u}_{i:}\\||_F^2 \n\\\\)\n\n\\\\(\n\\arg\\min_{\\mathbf{v}_{j:}} \\gamma(U,V) = \\arg\\min_{\\mathbf{v}_{j:}} \\||\\mathbf{y}_{:j} - \\mathbf{U}\\mathbf{v}_{j:}\\||_F^2 \n\\\\)\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 22,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\nLeast square problems &#8667; Unique solutions!\n\n\\\\(\n\\mathbf{u}_i \\leftarrow (\\mathbf{V}^T\\mathbf{V})^{-1} \\mathbf{V}^T \\mathbf{y}_{i:} \n\\\\)\n\n\n\\\\(\n\\mathbf{v}_j \\leftarrow (\\mathbf{U}^T\\mathbf{U})^{-1} \\mathbf{U}^T \\mathbf{y}_{:j} \n\\\\)\n\nEach step can be parallelized efficiently",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  }, {
    "id" : 23,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "reg",
      "extraFields" : { }
    }
  }, {
    "id" : 24,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Regularization\n\nObjective:\n\n$$\n\\gamma(U,V) := \\||\\mathbf{Y} - \\mathbf{U}\\mathbf{V}^{T}\\||_F^2 + \\lambda_1Reg1(\\mathbf{U}) + \\lambda_2Reg2(\\mathbf{V})\n$$\n\nCommon choices for \\\\(Reg1\\\\) and \\\\(Reg2\\\\):\n\n- sparsity-inducing 1-norm: \\\\(||\\mathbf{v_j}||_1\\\\)\n- overfitting-avoiding squared 2-norm: \\\\(||\\mathbf{u}_i||_F^2\\\\)\n\n> 1-norm not continuously differentiable<br/>\nOptimization harder but still possible",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "rlsi",
      "extraFields" : { }
    }
  }, {
    "id" : 26,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Regularized Latent Semantic Indexing\n\ndocument-term matrix \\\\(\\mathbf{Y}\\approx \\mathbf{U}\\mathbf{V}^T\\\\)\n\ndocument-topic matrix \\\\(\\mathbf{U}\\\\), term-topic matrix \\\\(\\mathbf{V}\\\\)\n\n<div style=\"text-align:center;\">\n<table>\n\n  <tr>\n    <th>\\(\\mathbf{U}\\)</th>\n    <th>\\(\\mathbf{V}\\)</th>\n    <th>Readability</th>\n    <th>Compactness</th>\n    <th>Retrieval</th>\n  </tr>\n  <tr>\n    <td>1-norm &nbsp;</td>\n    <td>2-norm &nbsp;</td>\n    <td>✓</td>\n    <td>✓</td>\n    <td>✓</td>\n  </tr>\n  <tr>\n    <td>2-norm</td>\n    <td>1-norm</td>\n    <td>&#10003;</td>\n    <td>&#x2717;</td>\n    <td>&#x2717;</td>\n  </tr>\n  <tr>\n    <td>1-norm</td>\n    <td>1-norm</td>\n    <td>&#10003;</td>\n    <td>&#10003;</td>\n    <td>&#x2717;</td>\n  </tr>\n  <tr>\n    <td>2-norm</td>\n    <td>2-norm</td>\n    <td>&#x2717;</td>\n    <td>&#x2717;</td>\n    <td>&#x2717;</td>\n  </tr>\n</table>\n</div>\n<br>\n<div style=\"text-align:right\">\n<i>Wang et al. (2011)</i>\n</div>",
      "extraFields" : { }
    }
  }, {
    "id" : 27,
    "compiler" : "section",
    "input" : {
      "sessionId" : null,
      "code" : "word",
      "extraFields" : { }
    }
  }, {
    "id" : 28,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "###Learning word embeddings\n",
      "extraFields" : { }
    }
  }, {
    "id" : 29,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "\nval layout = Layout(cw = 80, ch = 50, colHeaderSize = 150, rowHeaderSize=110)\n\nval Y = parseMatrix(\n    \"\"\"1 2 0 0 0\n       0 0 1 0 0\n       0 0 0 1 1 \n       0 0 1 1 1\n       1 2 1 1 0\"\"\")\n       \nval rowNames = Seq(\"context1\",\"context2\",\"context3\",\"context4\",\"context5\")\nval colNames = Seq(\"Greece\",\"Tsipras\",\"Germany\",\"crisis\",\"economy\")\nval names = header(rowNames,colNames)\nval rls = Matrix(hRulers = Seq(0,5), vRulers = Seq(0,5))\nval m0 = numbers(matrix(Y)) + names +rls\n\nval (u,v) = optimizeL2(Y,2,1000)\nval m4 = m0 + numbers(embeddings(u,v))\n\nrender(Seq(m0,m4),layout)",
      "extraFields" : {
        "showEditor" : "false",
        "aggregatedCells" : "[\"import uk.ac.ucl.cs.mr.acltutorial.ManualMF._\\nimport uk.ac.ucl.cs.mr.acltutorial.MatrixRenderer._\\nimport uk.ac.ucl.cs.mr.acltutorial.Renderer._\\nimport cc.factorie.la.{DenseTensor2, DenseTensor1}\\nimport ml.wolfe.{Mat, Vect}\\nval layout = Layout(cw = 80, ch = 50, colHeaderSize = 150, rowHeaderSize=100)\\n\\ndef optimizeL2(M: Mat, K: Int, iters: Int, alpha:Double = 0.1, initScale:Double = 1.0): (Seq[Vect], Seq[Vect]) = {\\n  val rand = new scala.util.Random(0)\\n  val AV = initialAV(K, M.dim1, M.dim2, initScale)\\n  val A = AV._1; val V = AV._2\\n  def update(i: Int, j: Int) = {\\n    val a = A(i).copy\\n    val v = V(j).copy\\n    val y = a dot v\\n    A(i) += v * alpha * (M(i, j) - y)\\n    V(j) += a * alpha * (M(i, j) - y)\\n  }\\n  for (i <- Range(0,iters).toList) {\\n    update(rand.nextInt(M.dim1), \\n           rand.nextInt(M.dim2))\\n  }\\n  (A, V)\\n}\",\"val Y = parseMatrix(\\n    \\\"\\\"\\\"1 2 0 0 0\\n       0 0 1 0 0\\n       0 0 0 1 1 \\n       0 0 1 1 1\\n       1 2 1 1 0\\\"\\\"\\\")\\n       \\nval rowNames = Seq(\\\"doc1\\\",\\\"doc2\\\",\\\"doc3\\\",\\\"doc4\\\",\\\"doc5\\\")\\nval colNames = Seq(\\\"Greece\\\",\\\"Tsipras\\\",\\\"Germany\\\",\\\"crisis\\\",\\\"economy\\\")\\nval names = header(rowNames,colNames)\\nval rls = Matrix(hRulers = Seq(0,5), vRulers = Seq(0,5))\\n       \\nval m0 = numbers(matrix(Y)) + names +rls\\nval m1 = opacity(matrix(Y), 0,2) + names + rls\\nval m2 = Matrix(Seq(Cell(2,2,\\\"Y\\\")), hRulers = Seq(5), vRulers = Seq(5))\\nval m3 = Matrix(Seq(Cell(6,2,\\\"V\\\"),Cell(2,6,\\\"U\\\"), Cell(2,2,\\\"Y\\\")), hRulers = Seq(5), vRulers = Seq(5))\\nval (u,v) = optimizeL2(Y,2,1000)\\nval m4 = numbers(embeddings(u,v)) +names\\nval Yhat = dots(u,v)\\nval m5 = numbers(matrix(Yhat)) + numbers(embeddings(u,v)) + names\\nval m6 = opacity(matrix(Yhat),0,2) + numbers(embeddings(u,v)) + names\\n\\nrender(Seq(m0,m1,m3,m4,m5,m6,m1),layout) \"]"
      }
    }
  }, {
    "id" : 30,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n- implicit MF: SkipGram (Mikolov et al. 2013)\n- explicit MF: GloVe (Pennington et al. 2014) and S-PPMI (Levy and Goldberg, 2014)",
      "extraFields" : {
        "fragment" : "true"
      }
    }
  } ],
  "config" : { }
}
